{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8478dbc",
   "metadata": {},
   "source": [
    "# Multitask-Zahlenerkennung mit Transfer Learning\n",
    "Dieses Notebook lädt handgeschriebene Ziffern (0–9) von drei verschiedenen Personen und trainiert ein Modell, das sowohl die Ziffer als auch die schreibende Person erkennt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ffe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import aller benötigten Bibliotheken für das Modell, das Training und die Datenverarbeitung\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigene Dataset-Klasse zur Verarbeitung der gespeicherten .npy-Bilder\n",
    "# Extrahiert aus Dateinamen die Ziffer und die zugehörige Person\n",
    "class DigitPersonDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # Manuelle Zuordnung der Namen zu Klassen-IDs\n",
    "        self.person_to_id = {'Tim': 0, 'Thanadon': 1, 'Nils': 2}\n",
    "\n",
    "        # Alle .npy-Dateien im Verzeichnis analysieren und passende Samples sammeln\n",
    "        for fname in os.listdir(data_dir):\n",
    "            if fname.endswith(\".npy\"):\n",
    "                parts = fname.split(\"_\")\n",
    "                if len(parts) >= 3:\n",
    "                    person = parts[0]\n",
    "                    digit = int(parts[1])\n",
    "                    person_id = self.person_to_id.get(person, -1)\n",
    "                    if person_id >= 0:\n",
    "                        path = os.path.join(data_dir, fname)\n",
    "                        self.samples.append((path, digit, person_id))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Bild und Labels laden\n",
    "        path, digit, person_id = self.samples[idx]\n",
    "        img_array = np.load(path)\n",
    "\n",
    "        # Fehlerbehandlung: sicherstellen, dass das Bild richtig formatiert ist\n",
    "        if img_array.dtype != np.uint8:\n",
    "            img_array = (img_array * 255 / np.max(img_array)).astype(np.uint8)\n",
    "        if img_array.ndim > 2:\n",
    "            img_array = img_array.squeeze()\n",
    "\n",
    "        img = Image.fromarray(img_array).convert(\"L\")\n",
    "\n",
    "        # Optional: Bildtransformation anwenden\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, digit, person_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition eines Multi-Task CNNs basierend auf ResNet18\n",
    "# Modell gibt gleichzeitig eine Vorhersage für Ziffer (0–9) und Person (0–2) aus\n",
    "class MultiTaskResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskResNet, self).__init__()\n",
    "        base = models.resnet18(pretrained=True)  # Vortrainiertes ResNet als Basis\n",
    "\n",
    "        # Ersetze den ersten Layer, um Graustufenbilder zu unterstützen (1 Kanal)\n",
    "        base.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Entferne den letzten Fully-Connected-Layer (wird durch zwei Köpfe ersetzt)\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1])\n",
    "\n",
    "        # Klassifikationskopf für Ziffern (10 Klassen)\n",
    "        self.head_digit = nn.Linear(base.fc.in_features, 10)\n",
    "\n",
    "        # Klassifikationskopf für Personen (3 Klassen)\n",
    "        self.head_person = nn.Linear(base.fc.in_features, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Durch das ResNet-Backbone\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        # Zwei parallele Vorhersagen\n",
    "        return self.head_digit(x), self.head_person(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformationen für Trainingsbilder: Resize, zufällige Rotation und Translation, Umwandlung in Tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Pfad zum Verzeichnis mit den .npy-Bildern\n",
    "data_path = \"C:/dhbw/6. semester/advanced ml/Zahlenerkennung-main/data/processed\"  # ggf. anpassen\n",
    "\n",
    "# Datensatz laden mit definierter Transformation\n",
    "dataset = DigitPersonDataset(data_path, transform=transform)\n",
    "\n",
    "# Aufteilung in Training (80 %) und Validierung (20 %)\n",
    "train_len = int(0.8 * len(dataset))\n",
    "val_len = len(dataset) - train_len\n",
    "train_data, val_data = random_split(dataset, [train_len, val_len])\n",
    "\n",
    "# Erstellen von DataLoadern für Training und Validierung\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6430299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auswahl des Geräts: GPU verwenden, wenn verfügbar, sonst CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Modell initialisieren und auf das Gerät verschieben\n",
    "model = MultiTaskResNet().to(device)\n",
    "\n",
    "# Verlustfunktion (gemeinsam für Ziffer und Person)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimierer: Adam mit Lernrate 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b5c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche  1 | Accuracy Digit: 11.39% | Person: 57.78% | Loss: 3.4606\n",
      "Epoche  2 | Accuracy Digit: 17.50% | Person: 69.72% | Loss: 3.0652\n",
      "Epoche  3 | Accuracy Digit: 17.78% | Person: 71.67% | Loss: 2.8783\n",
      "Epoche  4 | Accuracy Digit: 23.06% | Person: 79.44% | Loss: 2.6263\n",
      "Epoche  5 | Accuracy Digit: 29.17% | Person: 78.06% | Loss: 2.5826\n",
      "Epoche  6 | Accuracy Digit: 32.50% | Person: 81.67% | Loss: 2.3194\n",
      "Epoche  7 | Accuracy Digit: 38.06% | Person: 84.44% | Loss: 2.0210\n",
      "Epoche  8 | Accuracy Digit: 53.33% | Person: 87.22% | Loss: 1.7100\n",
      "Epoche  9 | Accuracy Digit: 52.78% | Person: 88.89% | Loss: 1.6979\n",
      "Epoche 10 | Accuracy Digit: 60.56% | Person: 85.28% | Loss: 1.5006\n",
      "Epoche 11 | Accuracy Digit: 66.94% | Person: 89.72% | Loss: 1.3421\n",
      "Epoche 12 | Accuracy Digit: 71.67% | Person: 89.72% | Loss: 1.1633\n",
      "Epoche 13 | Accuracy Digit: 75.83% | Person: 91.11% | Loss: 1.0145\n",
      "Epoche 14 | Accuracy Digit: 79.44% | Person: 89.72% | Loss: 0.9194\n",
      "Epoche 15 | Accuracy Digit: 77.50% | Person: 92.50% | Loss: 0.8474\n",
      "Epoche 16 | Accuracy Digit: 85.28% | Person: 92.22% | Loss: 0.7081\n",
      "Epoche 17 | Accuracy Digit: 87.50% | Person: 93.89% | Loss: 0.6156\n",
      "Epoche 18 | Accuracy Digit: 91.39% | Person: 92.22% | Loss: 0.5265\n",
      "Epoche 19 | Accuracy Digit: 91.67% | Person: 95.00% | Loss: 0.4524\n",
      "Epoche 20 | Accuracy Digit: 93.89% | Person: 93.06% | Loss: 0.4691\n",
      "Epoche 21 | Accuracy Digit: 92.78% | Person: 95.28% | Loss: 0.4102\n",
      "Epoche 22 | Accuracy Digit: 92.78% | Person: 93.33% | Loss: 0.3668\n",
      "Epoche 23 | Accuracy Digit: 94.17% | Person: 95.83% | Loss: 0.3551\n"
     ]
    }
   ],
   "source": [
    "# Überprüfung der NumPy-Version (optional)\n",
    "import numpy\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "# Training über 23 Epochen\n",
    "for epoch in range(23):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Mini-Batch-Training über den gesamten Trainingsdatensatz\n",
    "    for images, labels_digit, labels_person in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels_digit = labels_digit.to(device)\n",
    "        labels_person = labels_person.to(device)\n",
    "\n",
    "        # Vorwärtsdurchlauf und getrennte Verluste berechnen\n",
    "        out_digit, out_person = model(images)\n",
    "        loss_digit = criterion(out_digit, labels_digit)\n",
    "        loss_person = criterion(out_person, labels_person)\n",
    "        loss = loss_digit + loss_person  # Gesamtverlust\n",
    "\n",
    "        # Backpropagation und Optimierung\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Ausgabe des mittleren Epochenverlusts\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b54afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots fürs Training, genutzt zum Erstellen einer Grafik für die Dokumentation\n",
    "combined_train_acc = [0.5 * (d + p) for d, p in zip(train_digit_acc, train_person_acc)]\n",
    "combined_train_loss = [d + p for d, p in zip(train_digit_loss, train_person_loss)]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy-Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(combined_train_acc, marker=\"o\", label=\"Train Accuracy\", color=\"green\")\n",
    "plt.title(\"Gesamt Accuracy\")\n",
    "plt.xlabel(\"Epoche\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Loss-Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(combined_train_loss, marker=\"o\", label=\"Train Loss\", color=\"red\")\n",
    "plt.title(\"Gesamt Loss\")\n",
    "plt.xlabel(\"Epoche\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aefbcdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(loader):\n",
    "#     model.eval()\n",
    "#     correct_digit = 0\n",
    "#     correct_person = 0\n",
    "#     total = 0\n",
    "#     i=0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels_digit, labels_person in loader:\n",
    "#             i=i+1\n",
    "#             images = images.to(device)\n",
    "#             labels_digit = labels_digit.to(device)\n",
    "#             labels_person = labels_person.to(device)\n",
    "\n",
    "#             out_digit, out_person = model(images)\n",
    "#             pred_digit = out_digit.argmax(dim=1)\n",
    "#             pred_person = out_person.argmax(dim=1)\n",
    "\n",
    "#             correct_digit += (pred_digit == labels_digit).sum().item()\n",
    "#             correct_person += (pred_person == labels_person).sum().item()\n",
    "#             total += images.size(0)\n",
    "\n",
    "#     print(f\"Digit Accuracy: {correct_digit / total:.2%}\")\n",
    "#     print(f\"Person Accuracy: {correct_person / total:.2%}\")\n",
    "#     print(i)\n",
    "\n",
    "# evaluate(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f91f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erkennung pro Ziffer:\n",
      "Zahl 0:\n",
      "  → Ziffern-Erkennung: 75.00% korrekt\n",
      "  → Personen-Erkennung: 91.67% korrekt\n",
      "Zahl 1:\n",
      "  → Ziffern-Erkennung: 25.00% korrekt\n",
      "  → Personen-Erkennung: 91.67% korrekt\n",
      "Zahl 2:\n",
      "  → Ziffern-Erkennung: 41.67% korrekt\n",
      "  → Personen-Erkennung: 100.00% korrekt\n",
      "Zahl 3:\n",
      "  → Ziffern-Erkennung: 33.33% korrekt\n",
      "  → Personen-Erkennung: 83.33% korrekt\n",
      "Zahl 4:\n",
      "  → Ziffern-Erkennung: 58.33% korrekt\n",
      "  → Personen-Erkennung: 100.00% korrekt\n",
      "Zahl 5:\n",
      "  → Ziffern-Erkennung: 58.33% korrekt\n",
      "  → Personen-Erkennung: 83.33% korrekt\n",
      "Zahl 6:\n",
      "  → Ziffern-Erkennung: 66.67% korrekt\n",
      "  → Personen-Erkennung: 100.00% korrekt\n",
      "Zahl 7:\n",
      "  → Ziffern-Erkennung: 75.00% korrekt\n",
      "  → Personen-Erkennung: 100.00% korrekt\n",
      "Zahl 8:\n",
      "  → Ziffern-Erkennung: 41.67% korrekt\n",
      "  → Personen-Erkennung: 66.67% korrekt\n",
      "Zahl 9:\n",
      "  → Ziffern-Erkennung: 91.67% korrekt\n",
      "  → Personen-Erkennung: 100.00% korrekt\n",
      "\n",
      "Erkennung pro Person:\n",
      "Nils:\n",
      "  → Ziffern-Erkennung: 57.50% korrekt\n",
      "  → Personen-Erkennung: 90.00% korrekt\n",
      "Thanadon:\n",
      "  → Ziffern-Erkennung: 65.00% korrekt\n",
      "  → Personen-Erkennung: 100.00% korrekt\n",
      "Tim:\n",
      "  → Ziffern-Erkennung: 47.50% korrekt\n",
      "  → Personen-Erkennung: 85.00% korrekt\n",
      "\n",
      "Gesamt Ziffern-Accuracy: 56.67%\n",
      "Gesamt Personen-Accuracy: 91.67%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "\n",
    "# Pfad zur Evaluationsdaten und Zuordnung der Namen zu IDs\n",
    "eval_path = \"C:/dhbw/6. semester/advanced ml/Zahlenerkennung-main/eval\"\n",
    "person_to_id = {\"Tim\": 0, \"Thanadon\": 1, \"Nils\": 2}\n",
    "id_to_person = {v: k for k, v in person_to_id.items()}\n",
    "\n",
    "# Transformation für Eval-Bilder (nur Resize und Tensorumwandlung)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Statistiken initialisieren (für Ziffern- und Personen-Genauigkeit)\n",
    "digit_results = defaultdict(lambda: {\"digit_total\": 0, \"digit_correct\": 0,\n",
    "                                     \"person_total\": 0, \"person_correct\": 0})\n",
    "person_results = defaultdict(lambda: {\"digit_total\": 0, \"digit_correct\": 0,\n",
    "                                      \"person_total\": 0, \"person_correct\": 0})\n",
    "\n",
    "# Modell in den Evaluationsmodus setzen und Vorhersagen berechnen\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for fname in os.listdir(eval_path):\n",
    "        if not fname.endswith(\".npy\"):\n",
    "            continue\n",
    "\n",
    "        # Bild laden und vorbereiten\n",
    "        fpath = os.path.join(eval_path, fname)\n",
    "        img_array = np.load(fpath)\n",
    "        if img_array.dtype != np.uint8:\n",
    "            img_array = (img_array * 255 / np.max(img_array)).astype(np.uint8)\n",
    "        if img_array.ndim > 2:\n",
    "            img_array = img_array.squeeze()\n",
    "        img = Image.fromarray(img_array).convert(\"L\")\n",
    "        img = eval_transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        # Labels aus dem Dateinamen extrahieren\n",
    "        parts = fname.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        person_name = parts[0]\n",
    "        digit_label = int(parts[1])\n",
    "        person_label = person_to_id.get(person_name, -1)\n",
    "\n",
    "        # Modellvorhersage\n",
    "        pred_digit_logits, pred_person_logits = model(img)\n",
    "        pred_digit = torch.argmax(pred_digit_logits, dim=1).item()\n",
    "        pred_person = torch.argmax(pred_person_logits, dim=1).item()\n",
    "\n",
    "        # Statistik pro Ziffer\n",
    "        digit_results[digit_label][\"digit_total\"] += 1\n",
    "        digit_results[digit_label][\"person_total\"] += 1\n",
    "        if pred_digit == digit_label:\n",
    "            digit_results[digit_label][\"digit_correct\"] += 1\n",
    "        if pred_person == person_label:\n",
    "            digit_results[digit_label][\"person_correct\"] += 1\n",
    "\n",
    "        # Statistik pro Person\n",
    "        person_results[person_name][\"digit_total\"] += 1\n",
    "        person_results[person_name][\"person_total\"] += 1\n",
    "        if pred_digit == digit_label:\n",
    "            person_results[person_name][\"digit_correct\"] += 1\n",
    "        if pred_person == person_label:\n",
    "            person_results[person_name][\"person_correct\"] += 1\n",
    "\n",
    "# Ausgabe pro Ziffer\n",
    "print(\"Erkennung pro Ziffer:\")\n",
    "for digit in sorted(digit_results.keys()):\n",
    "    r = digit_results[digit]\n",
    "    digit_acc = r[\"digit_correct\"] / r[\"digit_total\"] * 100 if r[\"digit_total\"] > 0 else 0\n",
    "    person_acc = r[\"person_correct\"] / r[\"person_total\"] * 100 if r[\"person_total\"] > 0 else 0\n",
    "    print(f\"Zahl {digit}:\")\n",
    "    print(f\"  → Ziffern-Erkennung: {digit_acc:.2f}% korrekt\")\n",
    "    print(f\"  → Personen-Erkennung: {person_acc:.2f}% korrekt\")\n",
    "\n",
    "# Ausgabe pro Person\n",
    "print(\"\\nErkennung pro Person:\")\n",
    "for person, r in person_results.items():\n",
    "    digit_acc = r[\"digit_correct\"] / r[\"digit_total\"] * 100 if r[\"digit_total\"] > 0 else 0\n",
    "    person_acc = r[\"person_correct\"] / r[\"person_total\"] * 100 if r[\"person_total\"] > 0 else 0\n",
    "    print(f\"{person}:\")\n",
    "    print(f\"  → Ziffern-Erkennung: {digit_acc:.2f}% korrekt\")\n",
    "    print(f\"  → Personen-Erkennung: {person_acc:.2f}% korrekt\")\n",
    "\n",
    "# Gesamtauswertung über alle evaluierten Bilder\n",
    "total_digits = sum(r[\"digit_total\"] for r in digit_results.values())\n",
    "total_digit_correct = sum(r[\"digit_correct\"] for r in digit_results.values())\n",
    "total_person_correct = sum(r[\"person_correct\"] for r in digit_results.values())\n",
    "\n",
    "print(f\"\\nGesamt Ziffern-Accuracy: {total_digit_correct / total_digits * 100:.2f}%\")\n",
    "print(f\"Gesamt Personen-Accuracy: {total_person_correct / total_digits * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e6cc7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# eval_path = \"C:/dhbw/6. semester/advanced ml/Zahlenerkennung-main/eval\"\n",
    "# behaltene_endungen = (\"16.npy\", \"17.npy\", \"18.npy\", \"19.npy\",\"36.npy\", \"37.npy\", \"38.npy\", \"39.npy\")\n",
    "\n",
    "# # Alle Dateien im Ordner durchgehen\n",
    "# for fname in os.listdir(eval_path):\n",
    "#     if not fname.endswith(behaltene_endungen):\n",
    "#         fpath = os.path.join(eval_path, fname)\n",
    "#         if os.path.isfile(fpath):\n",
    "#             os.remove(fpath)\n",
    "#             print(f\"GELÖSCHT: {fname}\")\n",
    "#         else:\n",
    "#             print(f\"ÜBERSPRUNGEN (kein File): {fname}\")\n",
    "#     else:\n",
    "#         print(f\"BEHALTEN: {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modell erstellen und speichern\n",
    "torch.save(model.state_dict(), \"model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6eebc99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# state_dict = torch.load(\"model.pth\", map_location=\"cpu\")\n",
    "# print(\"\\n\".join(state_dict.keys()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
